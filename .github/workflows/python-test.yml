name: Python Test

on:
  push:
    branches:
      - main  # Run the workflow when code is pushed to the main branch

jobs:
  run-python-script:
    runs-on: ubuntu-latest
    environment: staging
    permissions:
      id-token: write
      contents: read
    steps:
      # Step 1: Checkout the repository
      - name: Checkout code
        uses: actions/checkout@v3

      # Step 2: Clone scraper repo
      - name: Clone scraper repository
        run: git clone https://github.com/ravilladhaneesh/github-data-processor.git

      # Step 3: Configure aws credentials
      - name: Configure aws credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: ${{ secrets.AWS_PUT_DATA_ROLE}}
          aws-region: ${{ secrets.AWS_REGION }}
          
      # Step 4: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'  # Set Python version (e.g., 3.8, 3.9)

      # Step 5: Set repository name as environment variable
      - name: Set ENV variable with repository name
        run: |
          echo "REPO_NAME=${{ github.repository }}" >> $GITHUB_ENV
          echo "REPO_PATH=${{ github.workspace }}" >> $GITHUB_ENV
          echo "REPO_URL=https://github.com/${{ github.repository }}" >> $GITHUB_ENV
          echo "BRANCH=${{ github.ref_name}}" >> $GITHUB_ENV
          echo "REPO_VISIBILITY=${{ github.event.repository.private }}" >> $GITHUB_ENV

      # Step 6: Run the Python script
      - name: Run scraper
        env:
          ROLE_ARN: ${{ secrets.AWS_PUT_DATA_ROLE }}
        run: |
          pip install -r github-data-processor/requirements.txt
          python github-data-processor/src/main.py  # This will run the github-data-processor project
